{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验5 HMAX模型的实现\n",
    "> 网上找资料找了很长时间，这个后面的几个层着实没咋看懂  \n",
    "> 最后从[**一篇学长写的博客**](https://blog.csdn.net/ZEBRONE/article/details/125112108)中发现github上有HMAX的模型定义代码，就拿来用了    \n",
    "\n",
    "[https://github.com/wmvanvliet/pytorch_hmax](https://github.com/wmvanvliet/pytorch_hmax)  \n",
    "- 这里的`hmax.py`文件里边定义了hmax的S1,C1,S2,C2层  \n",
    "- universal_patch_set有这个模型预训练的参数  \n",
    "- 我们只需要加载这些参数然后处理数据就行了  \n",
    "> 这里我感觉其是就是这几个层做一个特征工程，提取特征什么的，然后我们要实现的训练调参的就是最后的VTU层  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing model\n",
      "Running model on cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt  \n",
    "import hmax\n",
    "# Initialize the model with the universal patch set\n",
    "# Initialize the model with the universal patch set\n",
    "print('Constructing model')\n",
    "model = hmax.HMAX('./universal_patch_set.mat')\n",
    "#定义数据预处理函数  \n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.Grayscale(),\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Lambda(lambda x: x * 255)])\n",
    "#加载训练数据，测试数据  \n",
    "train_data = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
    "test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform)\n",
    "#迭代器\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE)\n",
    "# Determine whether there is a compatible GPU available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#这里直接方法奥CPU上训练，这样就可以无脑运行  \n",
    "#想用GPU的话改一下这里device还有下面的输出记得先转到CPU上在做其他操作  \n",
    "device='cpu'\n",
    "# Run the model on the example images\n",
    "print('Running model on', device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先展示一下处理过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data=train_loader.dataset.__getitem__(0)  \n",
    "plt.imshow(sample_data[0][0],cmap='gray')\n",
    "plt.show()\n",
    "all_layers=model.get_all_layers(sample_data[0].reshape(1,1,28,28))\n",
    "s1=all_layers[0][0]\n",
    "c1=all_layers[1][0]\n",
    "s2=all_layers[2][0]\n",
    "c2=all_layers[3][0]\n",
    "#S1层的某个波段的四个方向\n",
    "fig, axes = plt.subplots(1, len(s1[0]), figsize=(15, 5))   \n",
    "for ax, img in zip(axes, s1[0]):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.title('S1')\n",
    "plt.show()\n",
    "#C1层  \n",
    "fig, axes = plt.subplots(1, len(c1[0]), figsize=(15, 5))   \n",
    "for ax, img in zip(axes, c1[0]):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.title('C1')\n",
    "plt.show()\n",
    "#S2\n",
    "fig, axes = plt.subplots(1, len(s2[0][0][:4]), figsize=(15, 5))   \n",
    "for ax, img in zip(axes, s2[0][0][:4]):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.title('S2')\n",
    "plt.show()\n",
    "#C2输出的是一个向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用Hmax处理数据  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#经过S1，C1，S2，C2处理的输入到VTU层的数据    \n",
    "train_data_vtu=[]\n",
    "train_data_labels=[]\n",
    "test_data_vtu=[]  \n",
    "test_data_labels=[]\n",
    "print('处理数据')  \n",
    "for X, y in tqdm(train_loader):\n",
    "    c2=model.forward(X.to(device))\n",
    "    max_out=torch.max(c2,dim=1)[0].numpy().tolist()\n",
    "    train_data_vtu+=max_out  \n",
    "    train_data_labels+=[int(i) for i in y]\n",
    "for X, y in tqdm(test_loader):\n",
    "    c2=model.forward(X.to(device))\n",
    "    max_out=torch.max(c2,dim=1)[0].numpy().tolist()\n",
    "    test_data_vtu+=max_out\n",
    "    test_data_labels+=[int(i) for i in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VTU层使用SVM，线性核(比较快)  \n",
    "> 这里反正就耐心等吧，本来数据集也大(2分钟左右)  \n",
    "> 可以自己随便写一个网络训练或者其他的方法试试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.954\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel=\"linear\")\n",
    "model.fit(train_data_vtu,train_data_labels)\n",
    "print(\"Accuray:\",model.score(test_data_vtu,test_data_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
